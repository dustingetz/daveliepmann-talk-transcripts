# The Design of Datomic

* **Speaker: Rich Hickey**
* **Conference: Clojure/West 2012**
* **Video: [https://www.infoq.com/presentations/The-Design-of-Datomic/](https://www.infoq.com/presentations/The-Design-of-Datomic/)**


Thanks Alex so much for putting us together. It's great to be here again and hear everybody's stories about using Clojure and also to tell my own.

I know everybody's got the same question, which is what's going on with the hair. Somebody told me if I grew my hair like this and got a phone booth something interesting would happen, but I still haven't found the phone booth but no. Seriously, I'm trying to get into the Foo Fighters but they haven't called me back.

So today I'm going to talk about Datomic. It's a database that we built in Clojure, and I really want to talk about the ideas inside Datomic. It's not really about showing code or trying to sell it or anything. I'm going to talk about it from three perspectives: first, what problems were we trying to solve; second, what are the solutions to those problems, sort of in abstract; and third, a little bit about how we implemented those, and then a little summary.

In terms of problems — of course, all the problems turn into the same problem at some point, right, which is dealing with complexity. When I was first working on Clojure, my first year or second year on Clojure, I found this paper and it was very inspirational. It's called "Out of the Tar Pit," and the authors had sort of pegged the set of problems that I had also pegged. We were dying with this complexity, and most of the complexity is caused by state and the way we manage state or fail to manage state. They also identified control as the source of complexity, and by that they meant sort of imperative programming, or even functional programming being too explicit about control.

This paper was really inspiring and actually was the paper that caused me to stick to my guns and make Clojure immutable by default and all the data structures immutable. I mean I had all kinds of ideas in the air and I was right on the edge, but when I read this paper I'm like, well, let me just do it. So I credit it as very much an inspiration. But there's a lot of challenges in the paper; it's not a paper with answers, really, it's a paper that poses a problem statement. They proposed functional programming and declarative programming, as expressed by relational algebra, as sort of a recipe for getting out of the tar pit: writing programs that were easy to understand, that were fundamentally simple, that were free of the complexity of both state and control.

But there's this problem with the paper: it gets to this point where they talk about the state which you're going to interact with relationally, and then maybe your program gathers information or produces information, then it goes back and like this state magically appears and it magically can get changed. The paper really doesn't address it at all — it doesn't even propose answers. It's just like somehow this could get updated. For me that's this throbbing problem, because this is really where we end up. If we adopt a functional programming language or a functional style of programming, we're still stuck with process. We have some constructs in Clojure for dealing with process, but in the end usually process produces information. That information has to go somewhere. That somewhere is the database — and then the program has to get back out of the database and start dealing with it. That interaction of process outside the system and back is really a problem.

Everybody who's working with Clojure still knows how nasty it is to try to deal with the database. Datomic is about closing the loop and solving the process problem. Another problem we're trying to deal with is the lack of declarative programming. We've already seen this situation improve now, right? We have cascalog, we have core.logic. That kind of stuff is really important because declarative programming and logic programming is just another level up from functional programming. It's another further abstraction away from _how_ to do things and concentrated focused on what you want to accomplish, what the problem is.

Right now most people, if they do declarative programming at all, only do it when they write SQL. It's something that's over there: these servers know how to do this, they have these declarative languages. And they're very powerful. They're much better at manipulating data than our programming languages, including languages like Clojure. They're another level higher. You don't typically write by hand a parallel hash join. But a database server when given some declarative commands does it for you all the time. So there's a lot of value there.

But again one of the problems with all logic languages traditionally is they have this notion of an ambient basis. In other words, I'm going to issue a query. Against what? Where is that stuff? How did it get there, and when you look at it again, will it be different, and how do you know when it's going to be different and why, and can you go back to something? Prolog, Datalog, all these languages didn't really focus too much on the data side, they focused on the query side. Even the query languages we have again don't really focus on the data side. So how can we give this kind of programming a sound basis? That's one of the problems we're trying to solve.

In addition to the declarative programming being 'over there', we have a general problem with stuff being 'over there' at all. Client-server programming has a ton of built-in limitations that often you don't really get to see. The first is this basis problem. If I issue two queries in a row against a server, what has happened in the interim? You have no idea. You have no way to recover the same basis for asking two questions, which leads you to all this fear — you're afraid of issuing multiple round trips, so you try to pile on everything you might want to know into this request. The other thing is even to read anything you have to send it over there, so you end up with very complex queries in order -- in one shot -- to get everything you might need. And those queries burden these servers. So you're afraid of overloading them. You see it in little ways you don't even recognize. Like, in SQL you'll often ask a query and part of the query is actually answering your question — which records satisfy these constraints? — and another big gob of your query is about pulling out the fields you want for your reporting job, because you want to get it all done in one roundtrip. Those things have nothing to do with each other. You've just glommed them all into a query because that's how client-server works. So can we do that better?

The other thing we're looking at trying to solve is how do we embrace some of the advances that have been made recently in making scalable systems that are arbitrarily scalable and distributed. A question I had was what's possible? How much of the goodness of something like Bigtable or Dynamo can we leverage without giving up consistency? Because I think consistency is really important. If you don't have consistency you've now taken on a huge boatload of additional complexity. There are definitely cases in which you have to make that trade-off: you say I need arbitrary write scaling, I'm dealing with huge data, I'm going to have to make this trade-off. That's why these systems were built. But these systems are full of cool research and capabilities that don't necessarily have to be applied to the problem they were designed around. Can we get some of the best of both worlds? Can we get consistency? And if we do, how much scalability can we get at the same time?

Another advantage of these kinds of systems is their elasticity. As a rule they tend not to be driven by pre-configuration but they're much more dynamic. That dynamic nature is something also we'd like to get. Everybody who pursues a database other than a SQL database is partially looking for more flexibility in data representation. Everybody's tired of the rectangles and the rigidity there. People want to do sparse data, they have irregular data, they have hierarchical things, and so document stores have become popular for doing that. Multi-valued attributes are particularly nasty and everybody would like a nicer way to do that.

I think the biggest thing you want in pursuit of flexibility is long-range flexibility. Day-one flexibility is straightforward: I use this document store, I can stick anything in it. Okay. That's great. That seems flexible. But this isn't flexibility — flexibility is moving. Time passes and you have to remain flexible. That's where you can trip up, because if you've encoded any structure at all — and that could be a set of tables or what you chose to put in a document — you've now got this structure that your future changes need to contend with. That structure is something that's impeding your flexibility to change your system and to move it forward. I call that pervasion of structural rigidity. Does it get into your program and make your program harder to change?

Another thing we'd like to solve is the time problem. In general, I think databases don't actually deliver the words they claim to. They claim to be storages of memory and to have records, but before computers these things had meaning that was a lot stronger than the meaning we deliver typically. When we remember things we actually remember them — we don't replace our old phone number with our new phone number in our head in the phone-number spot. Memory is something that you keep around, and records are something that you keep as well. Many, many systems _have_ to do this. Many systems don't _have_ to do this [but] would like to be able to do this, because it really gives you a lot of power: you can audit things, you can potentially look at different points in time, you can really keep track of what's happened.

I mean, how many people have ever worked in a system where it wasn't really working that well and then more data got into the database and it started working? And no one could ever figure out why? No one ever bothered to go back because they actually couldn't. This is what it's like. So could this be better? How can we leverage time and actually get it right?

Finally, the last problem is how do we incorporate perception and reaction? Another big part of the "Tar Pit" paper was building systems that were reactive, that somehow knew when there was novelty so they could act upon it versus having to poll. This is something that's really difficult to do. Some databases have stuff built in — you can set up an event system or a queue system alongside to sort of get events. But in general there's this sort of inversion problem you have, where you all know about change is that it's happening over there, and if you really want to know that change happened you have to go look over there and have to keep looking over there. We'd like to do that without polling. On the other hand, ideally perception is a consistent thing: when the light bounces off you, barring some really nasty interference, what you get is a pretty stable view of what happened. That stability, that consistency of perception, is another thing that's really important if you want to build a decision-making system on the other end.

So we'll again go through the same points and look at how to do it. This is what we're trying to get to: we're trying to make applications that are really empowered. They're empowered to perceive what's going on in the world. They're empowered to react to the things that are of interest to them. They're empowered to remember anything that's of interest to them and to make their own decisions.

The idea behind Datomic is: can I design a system that does this, and what do I need minimally under the hood to facilitate this? Because it's pretty easy to understand what you'd want in the peers, but what do you need underneath that to make that go? That's glommed together right here with something I'm calling coordination services, but I'll split that out later.

So, the first thing we have to do is get coherent about what we mean when we say 'state'. This is the problem now: we're trying to incorporate process. We know how to do state in a program, right, we have values, the value is great. But then we know time passes and there's another value. When you want to put that in a database, what are you going to do? Traditionally databases have replaced things in place. I'm going to call that place-oriented programming. There's a lot of negative aspects to that. We need a different idea. We want to preserve the tenets of functional programming, but what does it mean to have a value that changes?

One way to think about it is if it only changes by expansion or accretion, like tree rings — can you see the tree rings back there — tree rings just keep going out. The inside is still there. It's like a value that gets bigger but it never actually changes in place. So there's this in-between world: there's stuff and then I change the stuff, that's update-in-place -- and what happened? Who knows, it was all over itself. Then there's stuff, more stuff. That's actually pretty easy to understand, because if I knew this and then you said that plus that, I can get a grip on that as a value.

There's a more complex way to talk about this, which is the value extends over time and you're just discovering it as time passes. It's like StarCraft: you're moving and you see more of the map. So you just your business more and you see more of your database. It's a way of thinking about it. Although, my wife warned me about the determinism problem of saying that. And now I've said it. But the fundamental idea is that the past doesn't change. That gives us something we can hook on to and build on top of. It's not exactly a value. It's not like 42, I mean, 42 doesn't get a little bit bigger. But it's a new notion of a value, where the core of it remains intact and all you can ever do is sort of grow it.

When you think about it this way you realize a few things right away, which is that process -- and process is what I mean by either genuine novelty (there's new information in the world) or what we would consider updates. But the word "update" is kind of bizarre. If you have a new phone number you didn't actually go to your old phone number and change some of the numbers. It's not really "update". We think of it as "update" because we're associating it with the same attribute of the same entity, but it's actually novelty. So process is about novelty.

When you take this idea of state you realize the first implication is new state means new space. We have to get new space. I don't think anybody with a tree in their backyard is worried about the trunk size of the tree over-running their house. There's actually not that much novelty in the world relative to whatever is already there. No one's growing their business at 10x per year for an indefinite amount of time. So we're going to accept that: process requires new space. It's a given. It's like, in Clojure, I said I want persistent data structures, that requires garbage collection. You have to sort of make that decision: okay, I'm going to accept that as a given and then design a system from there.

The other thing that's fundamental is we have to move away from places. Places destroy this entire idea. There's no way to have places and this idea of a value.

So let's talk about process now. Process is about novelty: there's either new information or information has "changed". (This is the way you say "changed", it uses four fingers [air quotes].) It's the same thing, "change". The first thing you need to do with process to build this system is to reify it. You need to make a thing out of it. What happens in an update-in-place system? You look at the world and it's like this, and you look later it's like that. What happened? Where is what happened? It's gone. It's just the effect of a bunch of independent things; it never really had a life of its own. So what we want is to reify process, make it something we can hold on to, touch, pass around. There's tremendous value in making novelty concrete. So we're going to do that.

The other answer to the process thing is that you need to find some representation for novelty. When people hear "new information requires new space" they're like, "Oh my God, if I change your email you're going to save the whole document again?" Well that statement has a ton of presumption in it. It presumes the only way you could possibly change somebody's email address is to store their entire document again. Who says that? It's not written down anywhere. It doesn't have to be that way.

One of the things we want to make sure of is when we grow our value and we represent the process — which is what's new — that process is minimal. A representation of novelty is minimal. It's as small as it can be. I'm not saying this is the only way, but one way to represent novelty is to say we're only going to represent data as facts, and the only way we're going to represent process is as the assertion or retraction of a fact. (We'll talk more about facts later.) Anything else that might happen in the world we can boil down into that, we can represent that way, and that becomes really small. "I now assert your email address is this." I don't care about your document. There is no document. And there's no more space required to say that than the amount of novelty.

For declarative programming, I think this is actually the easiest part. We've already seen this get added to Clojure and other languages — languages like Datalog. We're particularly going to choose Datalog, but there's nothing that says you can't have other or more query languages. The important thing is we want this in the application. We want to move this declarative power inside the application. We also want it to be integrated with the rest of the application, as opposed to being a dedicated system that only works about itself. We want to extend this programming so that it can apply to the data structures you have in memory and the rest of your program. You want queries to work on anything. And you want it to be extensible to user code. This is the way it really becomes part of your application: it can call your code and vice versa.

To solve the over-there problem, we have to move the data to neutral territory. One of the problems of client-server is it's based upon a model of hardware that's disappearing on us. There were these servers, they were really expensive, it was expensive to get a machine with big processors, big memory, and big disks. So it became a really important and rare thing.

So sending stuff over to the server made sense. Now, with SSDs and fast networks, the privilege of data access that was possessed by that server is no longer unique to it. It can be possessed by any machines. Therefore moving the data out of the privileged control of a single machine becomes a critical underpinning to building a system where you have peers and can spread around your computational load and your data access.

So we want no privileged access. This idea begs the question of, so what is storage? It's not actually a disk. There's not a lot of disks you can connect multiple processes to. Now there are SANs and things like that, and  there are systems built around that. That's a flavor of this idea: moving the data to neutral territory. But you can encapsulate the idea of a SAN -- and all these concepts of data being over there and universally accessible -- in the idea of a storage service. There's some service that has the storage, I don't know if it's one machine or ten machines or a hundred machines. I don't know where it is. I don't want to know anything. I just want to say, "Give me this block of information," and it says, "Here you go." Those services exist and some of them are very capable.

You want to take advantage of these services: no dedicated machines, no special accessors. We can always superimpose that, right? If, for some reason, for working set coherence, we want to have one machine only does the A's and one machine only does the B's, so we get more scalability -- you can always do that _over_ the data. You don't have to make an "A" machine and a "B" machine with an A disk and a B disk, right? You can make an A working set and a B working set and a storage service that everybody can access. You'll still get those benefits of locality.

So you want to separate. You know, this is the same thing we saw in Clojure. Most design is taking stuff apart.

We're going to call the applications "peers". At the point in time that they have as good access to data as any server ever had, they're peers in the system. The notion of a privileged server (for this purpose) goes away. So how can we take advantage of these scalable storage systems and redundant storage systems? The key is to actually separate reads and writes. As long as you're doing update-in-place, there's a place. You're putting stuff there. If someone wants to see what has happened they have to go there. And if they want to see something _consistent_ they have to go there and stop everybody else. Again, this place orientation is a killer, architecturally. It just stops everything. You can't do much better than that. There are very complicated systems that try to pretend everybody has a writable database locally that's update-in-place, but they're very expensive and very very complex, and not really delivering a lot of these benefits. So we want to separate these two things.

When you separate them, when you say, "writing happens over here, reading happens over there", you now can make independent decisions about the availability/consistency/scalability trade-off. That's what Datomic does: it says, "You know what, I'm happy with traditional scalability of consistent systems." In other words, I like transactional systems. I like transactional servers. I have yet to build a system that soaked the server. I know plenty of other people who are in the same boat: the only way these servers get soaked is with queries, almost never with writes. If you had a server that only handled writes you would be able to accommodate a lot of business and other problems. So we segregate writes and we use transactions for writes and we do only that there.

Then we make the output and the results of transactions be immutable. By making it immutable, we've solved the place problem, right? If I want to see something immutable, do I have to go where it was first made? Can I go to a copy? Sure! A copy is just as good as the original place. So we write our stuff someplace where we can make many copies. These storage services generally are highly redundant, they keep three or more copies. And then we can read that a la carte from anywhere at any time. So now the consistency problem -- you don't need transactions to deliver consistency for reads. You only need that because you were doing place-oriented programming. When you pull those two ideas apart you can make a different decision. So anybody who says "Oh, you have to do 'blah'," you say, "Are you talking about place-oriented programming? Because that's not the only kind of thing there is, or could be." Once you have immutable data you can move it around. Which also, once you've read something from somewhere, could you remember it? Sure! Are you gonna worry about that? "Ooh, I wonder if this is still good." No. You don't have to worry about it. As soon as somebody told you it's immutable you can cache it -- relentlessly -- in as many places as you want, as often as you want, wherever it makes sense.

In this way we get sort of both worlds with different trade-offs. We have consistency, which means yeah, we have availability limited to availability for single server solutions which can have hot stand-bys and the various traditional solutions. And then we can have real cool modern distributed storage for our reads. A little bit of both gives you an interesting system that's consistent and scalable on the read side, on the query side.

For flexibility we're just going to remove the structure. A good example of this is RDF. RDF said, we can represent anything in the universe with subject-predicate-object. They're almost right. That is almost right, just build it up.

What's a fact?

"Sally". Not doing it, right?

"Sally likes". Still not really a fact.

"Sally likes pizza". Sounds like a fact. It's like, ooh, that's a fact! But has she always liked pizza? Will she always like pizza? Now we're seeing that's not actually a whole fact. Maybe she was allergic to dairy and then she figured out how to deal with it and now she likes pizza. She _started_ liking pizza at a certain point in time. Then maybe she becomes allergic to tomatoes and she stops liking pizza.

Facts have time. So we have to become atomic but we have to make sure we're complete. Our notion of this we just call a datom, mostly because the plural of "datom" with a "u" is "data" and that's just a generic term that means nothing now. So we can have "datoms" if we spell it this way. It also makes for a cool, trademarkable name.

[A datom] will consist of entities, attributes, values, and some representation of time (which I'll talk about on the next slide). The fundamental thing is if it doesn't have time it's not a fact —- it's as incomplete a fact as "Sally." You need something not there.

How do we deal with time? One way would just to put a time-of-day timestamp on every fact, but it ends up that — as you'll see in the rest of the system — because transactions are serialized, they're as good a universal timeline as anything else. So we can instead put on the datoms instead of putting the time-of-day we can put the _transaction_ they were part of, and associate the time-of-day with the transaction. As soon as you do that, you're like, "ooh, if I could associate the time-of-day with the transaction, I might want to associate who did it, or what process did it, or where we did we get this information from with the transaction". And you say, "You know what? There's not a good reason for transactions not to be first-class entities."

So do it: transactions are first-class, and time is just an attribute of the transaction — as could be other things. Then you get to the critical point: we talked a couple of times about basis. You now have a notion of a basis for computations, which is the database value at a point in time. If we can figure out how to deliver that, we can really deliver the time promise. And it was an objective of mine in this design because I've made a lot of systems in the past that had to manipulate time; they had to keep everything, they had to keep the time of everything. Anybody ever write a system like that, right, you start adding timestamp fields or whatever? How is that query for finding the latest value at "now"? You like that query? How does that perform? How many people have tuned that query? Yeah. That's hard! That’s really hard. And then, once you build a system that has this, what do you end up with, often? So much of your system needs "now". So you build a whole bunch of your system that's "now". Because you can't say what "now" is: you're just like, "I want to ask my query; I want the freshest answer." So now you have processes and methods in your program or functions, and you have stored procedures that are all the "now" ones. Then you say, "Well, we stored those timestamps so we could do historical work and auditing. I want to ask those same questions as of last week." And you're like, "ugh!" Now you need another version of all of that, parameterized by time! And I have to go into all my joins and flow around that time basis. I've done that. I think that's incredibly difficult to get right and does not perform well.

So the idea is: instead of saying, "make time a parameter of everything", if instead you could say, "Look, just give me the value of the database as of last week." If there was some way to do that, then the same queries and the same programs could just be handed last week's database and would work the same. And that problem about that bug we had before we had a certain amount of data? We could go back and look at it: when was that happening? It was happening at 11 o'clock. Give me the database as of 11 o'clock — let's see it. Oh look, there's a problem; here's the problem in our code. It's fixed. The next time that won't happen to us. So we're going to shoot for that.

As far as perception and reaction: the idea is we've reified process. However we represent change, we're able to push it around, and that's all we're going to do. It's going to be data and we're going to push it. Once I've pushed you some data, you've got a query engine that can query arbitrary data. If you want to react to specific things, what do you do? You just query the novelty. And you can do that.

It ends up that the novelty alone isn't enough to answer all the kinds of reactions you want to have. Like, you might want to see, this stuff was new and what was the basis for it? Or this stuff was new and where did it take us? So it would be nice if we could get the values of the database before and after the processed event.

So all that sounds good. How do we do this? This [referring to diagram on screen] is the overview of the architecture. And this is the full production system. Because it's all defined in terms of protocols and whatnot, you can fold this up and run it in a tiny in-memory thing, but the logical model is this: your application is a process; it's going to incorporate a library that empowers it to be a peer. That library will have some communications bits (obviously) because it has to talk to the service parts of the system. It will have a query engine — I'll talk more about the live index and caching — and that goes in your application. Then there's this dedicated machine (or it could be a set of machines acting in standby for each other) called the transactor, whose job is only to handle transactions and writes.

So we're going to split reads and writes apart. We want a consistent world, so we want transactions. Essentially, what a transactor is, is like a database server where you took away their query -- well, they can do internal queries, but you took away their serving queries for others. You took away their serving reads for others. You took away their having to manage storage explicitly. What they're left with is handling transactions and some occasional background indexing. So that's all the transactor does: it just coordinates transactions. It accepts all the transaction requests, serializes them, and puts them in storage.

Finally, it has this storage service -- DynamoDB is the first one we support, but it's an _example_ of a storage service. You can look at a lot of things...you know, there's the Riak guys out there, I hope someday we can work on top of them. They have a really good storage service! It's a service, it runs as a cluster, it's redundant, it's distributed, it has all those great properties of DynamoDB. The idea here is, if you built it on a service model the interface to the services I'll talk about is pretty small. You can make independent choices! Again, you're moving away from something monolithic, you're going to end up with more choice: choice about location, choice about scale, choice about price. But DynamoDB is an example of a storage service. We put off all the actually sticking-bits-on-disks on this service.

And the peer, as you can see, can directly read back from the service. So the transactor will put stuff in storage, the peers will read it directly from storage. They don't read it back from the transactor.

The only interesting thing the transactor also does is you'll see this line back from the transactions to the peers: it also reflects the process. It reflects the change back to any connected peers. We'll see how that works in a second. So that's the overview of the architecture.

The first problem we encounter in trying to represent this is, how do we represent this immutable expanding value? What's the data structure for an immutable expanding value? [pause] It's tricky, right?

The first thing is it has to be organized. Why is the filesystem not a database? Or is it a database? Most people, when they buy databases, they're not like, "Oh now I can replace XFS with this database" or vice-versa. Usually the database adds some more value to "give me whatever is stored at key or filename X". It's _organized_. It's organized in such a way that it can help you _leverage_ the information you have. One way to leverage the information you have is to support queries, for instance.

So we want organized state, and we want it to be expanding. So, one representation of that is a sorted set of facts. Meets this criteria, right? "Your email was this", "your email is now that" because those have times, we can just add more facts, right? You change emails, you like new foods --  just add. We have the additive process, because facts have timestamps, they're going to be different from each other. But they have to be sorted in order to support query. What everybody has discovered in the past -- and BigTable is an example of a solution to this -- is that sorting live into storage is a bad idea. It's too expensive, has tremendous overhead for the actual write volume, the coordination, and everything else. And it's, like, place-oriented programming at its worst is actually maintaining a sorted index live on disk.

So, BigTable, the design is really simple. It says you should have a bifurcated system. You should accumulate novelty in memory, and periodically merge it to disk, and then you can keep a sorted version of things on disk. BigTable does this with big flat files, and what they say is: in order to see a sorted view of the world at any point in time, you're gonna have to merge. You're going to merge your memory view with your periodically-created, storage-based view in order to get a coherent view of now. So this merging is an efficient way to do this, and BigTable is a great example, and was an influential paper for this design.

So you want to accumulate novelty in memory. You want to occasionally merge it into storage in a sorted manner. The difference between what Datomic does and what BigTable does is, Datomic uses trees, unsurprisingly. Because what we want in memory is not some mutable thing. We want an immutable thing. We know the right way to represent that is with a tree. Similarly, in order to get the caching and addressable characteristics you want from putting your data on storage that the peers can access — you really don't want gigantic flat files. It's hard to say "I've cached this portion of this flat file," and it's hard to do multiple indexing jobs and have any portion of that caching you did still be valid, because the files sort of slid around. If instead you store a tree of nodes in storage, then I could remember this node, and if that node hasn't changed in the new indexing job, I can still remember it. So we use persistent trees in-memory and 'persistent-persistent', or durable persistent trees in storage, and basically implement the same idea: accumulate in memory, periodically merge into storage.

So what goes in storage? Obviously, novelty comes in. That has to be recorded right away if you want to meet ACID. It has to be durable before the transaction returns. So there's a log created of just the asserts and retracts. It's again unlike traditional logs in that it's not one contiguous file. Again, that whole idea of one contiguous file — that's super oriented around spinning disks on one machine. That's ending. It's just over. There's no reason to do that. A better representation is a tree again, for the same reasons: caching, locate-ability, and things like that.

So we log directly into storage. The other thing that's in storage are these index trees, and they're covering. In other words, each datom is entity, attribute, value, and transaction. Every index is a sorted view of all of that, so it's not really an index like an index that points at something else. A covering index has all the data you need inside it, so it's basically multiple sorted sets of the data, sorted in different ways as trees. Because we want this to work with services, you want to minimize the footprint, the API you use to talk to storage.

And so, in our case, what we're fundamentally based around is a storage that essentially implements key/value. We're not going to use the values to store, you know, "pizza". That's too tiny. We're gonna use the values to store entire segments of our index trees. So essentially we're using it as keys to blobs of index storage, much the same way that a traditional database would store blocks of its B-tree on the file system as blocks. We store blocks in the key/value store. But it ends up that fully implementing the state model requires something a little bit more than just "give me what's at this key and value," potentially inconsistently, which is the way a lot of these key/value stores work.

In order to emulate atoms, which is part of the model, we need consistent read, optionally, from these storage services. They need to be able, when required, to ask for something and make sure we get a consistent value back. A lot of these systems can do that. By turning up the number of reads you issue on a request you can ensure that what you get back is consistent.

The trickier thing which is less readily available in key/value stores right now, is conditional put. This is used by the Datomic storage engine for the moral equivalent of pods, which I'm so reluctant to say that word, but effectively that's what Datomic has inside of it. In order to accumulate log, you need something that allows you to build up the tail of what's going to become the new part of the tree. If you look at the inside of Closure's arrays, vectors, that's how they work, right? They build up a tail, and only when the tail is full do they pay the cost of merging it into the tree. So you need something like that, and in order to do that -- potentially safely, from dealing with multiple transactors succeeding each other -- you need conditional put. This is something that DynamoDB has; it's something I'm looking forward to being present in more of the key/value stores so that we can work on top of more of those.

So index stores look something like this. You have to read the details, but it's a tree, right? Up at the top there's a root that has the datom sorted by entity, then attribute, then value, then time; then the same datom sorted by attribute then entity then value then time. Potentially a reverse index for any reference-based attributes. We also do Lucene, we keep in the same way.

And then there's a three-level tree: a root, an intermediate set of directories, and a set of data segments. Inside each data segment are sorted datoms, and it's those segments and those tree segments that we put into storage — entire blocks like that. That's the granularity of what gets stored.

A database value is essentially a pointer to a couple of things. It's a pointer to the live tree in memory, which represents everything that's happened since the index on disk was made. So you have a persistent in-memory sorted set that you merge with the one off disk; so you have a pointer to the memory one and a pointer to the disk one. There's a lot of other subtleties in there with history and things like that, but the fact is the disk-based one is lazy, right? Obviously, you're not going to pull the entire universe off storage into your process if you don't care about it. So effectively you have a proxy for the storage engine sitting there saying: if you ask me for something I don't have, I'll go get it; I'll pull it into cache, and then manage your working set in cache. The cache is hierarchical — I'll talk about that, I think, in a second.

How do we do process itself? Well, one of the things is asserts and retracts can't actually express everything, right? I can't increment with assert: if it was 42 and I want to make it 43, I can't just — I could do the calculation somewhere, but I can't express the calculation by saying 43. So we have another notion of what constitutes part of the input to a transaction. It's called a data function. It's actually a function of the entire database inside the transaction, plus any arguments. What it outputs is another transaction segment. So a transaction can consist of assertions, retractions, and data-function calls. Data-function calls, in turn, will be passed the database and can return assertions, retractions, and data functions, and we expand and splice in the results until it's all asserts and retracts. So it looks like this [on-screen diagram], right?

You can have a transaction that was a certain assert, assert, retract, a call to the data function foo, and retract, assert, assert. Then foo could have expanded into calls to data functions bar and baz, and bar and baz could then, in turn, expand eventually into assertions and retractions. What does this look like? Macro expansions! That's right. This is the process version of macro expansion, but it's really cool because you end up with these primitives. You do end up with a primitive representation of process on which you can build something that allows you to do transformational updates.

We talked about the transactor, right? It has a couple of jobs: it accepts transactions. Those transactions look like what I just showed you. It does the job of expanding them. It will apply them to the in-memory version of the database. If nothing bad has happened, then that's a successful transformation of the database, at which point in time it will log it to storage and broadcast it back out to the other peer saying "this happened." What gets broadcast are the final assertions and retractions, so the peers don't have to do all the transformations again. They just get the answers sent to them. And periodically, the same transactor will do indexing — that's something we could move to another machine, but right now the transactors do it.

An interesting thing about this system though -- again we've seen parallels, I would hope constantly to the memory version -- is that, indexing is going to create at least a new root, probably some new directory nodes, and definitely a whole bunch of new data segments. What about the old ones? They're not getting updated, right? So where are they? They're still sitting there. They're now garbage. So you now have garbage collection in storage. It shouldn't be surprising: you have analogous things happening in storage that you had happening in memory, and there's no problem with that. So there's garbage and there's storage-based GC, that cleans it up later. When no one ever cares about that root anymore, it can go away (or on a time basis).

Declarative program we do by embedding Datalog. A couple of things about doing Datalog well for this purpose, especially for having a different basis, it's typical of languages like Datalog and Prolog that the database is kind of ambient, like you're just writing these things, and it's as if the database is always there. But that's essentially global, which means you have no way to talk about "I wanted to issue a query on _this_ data."

So we have to make sure that both the data sources for queries and the rule sets that are used in queries are _arguments_ to queries. They're not ambient and not global. When you ask a query of SQL, do you get to say "on what version of the data do you run?" No. It's like whatever's happening right then — your query runs right then. In the middle of the server, whatever is happening, it's always now. So if you wanted to be other than now, it has to be an argument, so they're arguments. We've extended Datalog so it can work with scalar values and collections and memory and things like that, and you can extend it with your own code.

So now we don't have the "over there" problem; now we have "over here," right? We can directly access storage. We have our own query engine. We have this live index. When the transactor pushes process back out to us, we update our own in-memory index. Just like the transactor is accumulating an in-memory index and then eventually merges it with this storage one, a peer is accumulating the same in-memory index and referencing the same stored one until the transactor says "I've made a new stored one," and you can drop what you've been accumulating in memory and move over to that as your stored one and we start from scratch here. Just build up a window of stuff, merge it, then drop it, build up a new window of stuff, merge it and then drop it. That's how it works.

Inside the peers, there's a two-tier cache. Eventually you end up with a ton of datoms. You can really overwhelm your Java heap by having, like, a gazillion objects as your cache. So we instead have a two-tier cache where stuff we've already pulled from storage we actually keep in the lowest tier of the cache as compressed, indexed segments that look to the Java heap as if they were just byte arrays. Only in a higher level of the cache do we turn them into, you know, thousands of objects. So there's a level that's thousands of objects or millions of objects, and a level which is compressed, indexed, not-yet-expanded guys. It's faster to just expand them than to go across the network to recover them. So you have these two tiers: one's on-heap and one is potentially off-heap, but at least is not high pressure. So we're now starting to get the consistency and scale stuff we were looking for. The writes go through the transactor. It has a traditional model of scalability. You can scale as big as one machine can scale. But that server is no longer burdened with any query load or read load. It's not indexing live on the same cores that it's using to serve transactions. And it has a traditional availability model: you run hot standbys, and they're easy to run because their reference data is not like a live connection from the other server — it's the storage. So it's very easy to transition from one to the other. You don't have these complicated "shoot-the-other-guy-in-the-head" relationships between streaming servers.

The immutability is all we need to support consistent reads. So we get consistent read, but we also get scalable, consistent read because we can now use these highly scalable, redundant storage services, which is really a great combination. That's sort of what Datomic is about: getting this combination of features. If you don't need transactions, it may not be the right thing, but if you do, it's an interesting combination.

And query scales with peers, right? Your computational load is now on a peer. If you want to run a long-running query and you have your own box, okay, that computation is not interfering with other computations. So having a set of peers that just do analytics is not really in anybody else's way, which is part of the objective.

In terms of the flexibility side, what do we have for schema? Well, we boil it down: we said that all we're going to store are these little facts. In fact that's the end of the representational structure. There's no other structural things; there's datoms, that's what we store. So the only thing you do have to encode are your attributes, and it's important that you do this right. There are systems that say "just stick in whatever you want; just put any text; we'll just take all your text," and they redundantly store all the text; they're done. And, you know, they take all your typos; they don't understand the types of things. It's not really that great as a database. It's worth spending this much effort to say, you know what: the name attribute is a string, and the time-of-day attribute is a date, and the other thing is a time. So you have to tell us the name and type, and then cardinality.

Cardinality is a really interesting problem, right? Think about your typical database: how much different is it to say "Fred has a new email", and "Fred has a new friend"? It's different, right? Why is it different? I don't know. I don't think it's a good idea that it should be different. I think it should be the same. So that's what we support: you tell us what the cardinality is, and if you tell us emails (cardinality one) when you say Fred has a new email, that's the email we're gonna return when you ask. If you say Fred has a new friend, that friend will be included in the _set_ of friends we returned when you asked. And that's how I think it should work.

Think about a relational database: what happens with those two things? Email may go right in a record, but friend definitely has to go in a different table, right? Even in a document store, same kind of problem, right? Email is like directly in the attribute, and friend is in what?  I don't know. If it's JSON, it's in a list, right? Which means that making that change to add a new friend means changing that list. That can become precarious, and that's why things like Redis have smarter primitives for that. Because that's ugly if you don't do that better.

Another notion of an attribute is whether or not it's a component, right? Your arm, if it was in a separate entity, is a component of you; your grandmother is not. The grandmother is —- so both are entities, possibly depending on what you're doing, but one is definitely part of you. If you were to go away, your arm would go away too, and the other is a relationship. So we want to know that. We deal with uniqueness; we also have a way to talk about things by name, which matters. Effectively it gives you multimaps.

So, for time — we talked about this a bunch of times, but — the database ends up being a value. You can issue queries across multiple databases, or a database in the past, and a database from now. But the critical thing is you can take a database and say: given this database, I'd like to see this database as of two weeks ago. Once you do that, you have another database value you can place into a query and get answers as of two weeks ago. Similarly, you can do "since" a point in time. The other thing we can do is we can do "as if," right? You can ask for this database with this transaction applied, not going through the transactor; it's just a value transformation. What would this database look like if I issued this transaction on it? Let you do what-if stuff without interacting with the rest of the universe or interfering with it, which is really a big deal.

There's also an efficiency thing here in how we deal with time. We move the past, any retracted past, into a separate index so it's not in your way.

Finally, for perception-reaction, this is actually straightforward now, right? We saw there's already a live feed of process from the transactor to the peers. You can basically just tap into that in your program and say, "I'd like a queue of those process events," and then you can take them. What we pass actually is the process event, the database before it and the database afterwards, and you can issue any queries you want in order to trigger any activity you find interesting.

So, to summarize: I think the state-time-identity model should be familiar to this audience. I really didn't set out to make it the same. I wanted to make sure that I just answered these questions. It ended up being the same, which I think is great, but obviously state-time-identity is not enough to solve the database problem. You need this process, and so that's what really you add: you have a reification of process and the ability to manipulate it, an ability to get values of databases. From a technical standpoint, that dynamic merge thing that BigTable does — it's essential. You cannot store persistent data structures on disk live. You can try it; I mean, you can technically do it, but it will always perform terribly and use up a ton of space. So it's a good idea that we copied.

The other thing, is just again, how many times did I say immutability in this talk? Immutability rocks. One of the things that's very interesting about it is you cannot represent change without it. You can't correctly represent change without immutability. It's a profound idea; it's not my idea. I think it's just some essential characteristic of the universe, but it really needs to be recognized in our architectures. So, if I had any recommendation to you at all, it's just: if you think about designing systems and you're not sure you can answer all these questions in the forward direction, choose immutability. You could almost back into a little bit more than 50% of this design just by having taken immutability as a constraint and saying, "Oh my God, now what am I gonna do? I'm not allowed to change this; I better do this." It'll keep forcing you into good answers.

So if I had any sort of architectural guidance from this, it's just: do it. Choose immutability and see where it takes you. That's it.

[Applause]
